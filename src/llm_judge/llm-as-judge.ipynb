{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737c668-568f-47e4-9128-f7074df5c71e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet ipywidgets accelerate bitsandbytes huggingface_hub transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7530b93a-100b-49f7-b653-aa6698ceba07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3106dc3a98e47c8a2dbb496ae331b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "MODEL_VARIANT = \"27b-chat\"  # @param [\"2b-predict\", \"9b-chat\", \"9b-predict\", \"27b-chat\", \"27b-predict\"]\n",
    "\n",
    "model_id = f\"google/txgemma-{MODEL_VARIANT}\"\n",
    "\n",
    "if MODEL_VARIANT == \"2b-predict\":\n",
    "    additional_args = {}\n",
    "else:\n",
    "    additional_args = {\n",
    "        \"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)\n",
    "    }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,  # âœ… allow offloading to CPU\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\", \n",
    "    quantization_config=bnb_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a291f1-1b15-4c7e-9fcf-77d4e3e9296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaabae4a-add2-4cf8-b0ed-182e21f8ccc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instructions: Answer the following question about drug properties.\n",
      "Context: Dabrafenib is a small molecule drug\n",
      "Question: Find me the drug information about this Dabrafenib\n",
      "Answer: Dabrafenib is a potent and selective BRAF kinase inhibitor. It is used in the treatment of melanoma and other cancers with BRAF gene mutations. \n",
      "\n",
      "\n",
      "Instructions: Answer the following question about drug involvement in pathway.\n",
      "Context: Dabrafenib is a potent and selective BRAF kinase inhibitor. It is used in the treatment of melanoma and other cancers with BRAF gene mutations.\n",
      "Question: Show me the most relavant pathways involvement for Dabrafenib\n",
      "Answer:\n",
      "BRAF Signaling Pathway\n",
      "PI3K/AKT Signaling Pathway\n",
      "MAPK Signaling Pathway\n",
      "\n",
      "Instructions: Answer the following question about drug effects.\n",
      "Context: Dabrafenib is a potent and selective BRAF kinase inhibitor. It is used in the treatment of melanoma and other cancers with BRAF gene mutations.\n",
      "Question: The genes ['ENSG00000283959', 'KLKP1-1', 'BBOX1-AS1', 'ENSG00000257732', 'ENSG00000285708', 'ENSG00000287682', 'MICOS13', 'ENSG00000286076'] are all upregulated, predict the effect of Dabrafenib\n",
      "Answer: Dabrafenib is a BRAF kinase inhibitor, so it is likely to downregulate BRAF and related genes. The provided genes are all upregulated, suggesting that Dabrafenib may have an opposite effect than expected. Further investigation is needed to understand the mechanism behind this observation. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "drug_list = [\"Dabrafenib\", \"Methotrexate\", \"Delamanid\", \"Demeclocycline\", \"5-Fluorouracil\"]\n",
    "drug = drug_list[0]\n",
    "\n",
    "# Stage 1: Get drug information\n",
    "prompt = f\"\"\"\n",
    "Instructions: Answer the following question about drug properties.\n",
    "Context: {drug} is a small molecule drug\n",
    "Question: Find me the drug information about this {drug}\n",
    "Answer:\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "\n",
    "# Extract drug information from the first response\n",
    "drug_info = response.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Stage 2: Get pathway involvement\n",
    "prompt = f\"\"\"\n",
    "Instructions: Answer the following question about drug involvement in pathway.\n",
    "Context: {drug_info}\n",
    "Question: Show me the most relavant pathways involvement for {drug}\n",
    "Answer:\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "\n",
    "# Stage 3: Predict drug effect given gene upregulation\n",
    "genes_list = ['ENSG00000283959', 'KLKP1-1', 'BBOX1-AS1', 'ENSG00000257732', 'ENSG00000285708', 'ENSG00000287682', 'MICOS13', 'ENSG00000286076']\n",
    "prompt = f\"\"\"\n",
    "Instructions: Answer the following question about drug effects.\n",
    "Context: {drug_info}\n",
    "Question: The genes {genes_list} are all upregulated, predict the effect of {drug}\n",
    "Answer:\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c5d6d6-8307-4cc5-b603-de014134d6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instructions: Answer the following question about drug properties.\n",
      "Context: Ponatinib is a small molecule drug\n",
      "Question: Find me the drug information about this Ponatinib\n",
      "Answer: Ponatinib is a potent inhibitor of the tyrosine kinases Bcr-Abl, Lyn, and Flt3. It is used to treat chronic myeloid leukemia (CML) and acute lymphoblastic leukemia (ALL).\n",
      "\n",
      "Instructions: Answer the following question about drug involvement in pathway.\n",
      "Context: Ponatinib is a potent inhibitor of the tyrosine kinases Bcr-Abl, Lyn, and Flt3. It is used to treat chronic myeloid leukemia (CML) and acute lymphoblastic leukemia (ALL).\n",
      "Question: Show me the most relavant pathways involvement for Ponatinib\n",
      "Answer:\n",
      "* **Tyrosine Kinase Signaling Pathway:** Ponatinib directly inhibits three tyrosine kinases: Bcr-Abl, Lyn, and Flt3. These kinases play crucial roles in various cellular processes, including cell growth, proliferation, differentiation, and survival. \n",
      "* **Leukemia Development:** Ponatinib's therapeutic effect stems from its ability to disrupt the aberrant signaling activity of these tyrosine kinases, which is often dysregulated in leukemia cells. By inhibiting these kinases, Ponatinib\n",
      "\n",
      "Instructions: Answer the following question about drug effects.\n",
      "Context: Ponatinib is a potent inhibitor of the tyrosine kinases Bcr-Abl, Lyn, and Flt3. It is used to treat chronic myeloid leukemia (CML) and acute lymphoblastic leukemia (ALL).\n",
      "Question: The transmembrane transporter binding activity is downregulated, and Arachidonic acid metabolism is upregulated, is it the same effect as the Ponatinib MOA?\n",
      "Answer: No. The provided information describes different molecular mechanisms. Ponatinib's mechanism of action involves inhibiting tyrosine kinases, while the information provided describes alterations in transmembrane transporter binding and arachidonic acid metabolism. These are distinct pathways and do not represent the same effect as Ponatinib's MOA. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "drug = \"Ponatinib\"\n",
    "\n",
    "# Stage 1: Get drug information\n",
    "prompt = f\"\"\"\n",
    "Instructions: Answer the following question about drug properties.\n",
    "Context: {drug} is a small molecule drug\n",
    "Question: Find me the drug information about this {drug}\n",
    "Answer:\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "\n",
    "# Extract drug information from the first response\n",
    "drug_info = response.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Stage 2: Get pathway involvement\n",
    "prompt = f\"\"\"\n",
    "Instructions: Answer the following question about drug involvement in pathway.\n",
    "Context: {drug_info}\n",
    "Question: Show me the most relavant pathways involvement for {drug}\n",
    "Answer:\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "\n",
    "# Stage 3: Predict drug effect given gene upregulation\n",
    "# genes_list = ['ENSG00000283959', 'KLKP1-1', 'BBOX1-AS1', 'ENSG00000257732', 'ENSG00000285708', 'ENSG00000287682', 'MICOS13', 'ENSG00000286076']\n",
    "prompt = f\"\"\"\n",
    "Instructions: Answer the following question about drug effects.\n",
    "Context: {drug_info}\n",
    "Question: The transmembrane transporter binding activity is downregulated, and Arachidonic acid metabolism is upregulated, is it the same effect as the {drug} MOA?\n",
    "Answer:\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5be17f5-0325-4843-8579-54e5acafe049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ¢ Computing for pre-loaded model: google/txgemma-2b-chat\n",
      "#valid tokens: 81\n",
      "Confidences mean/std: 0.8306 / 0.2305\n",
      "Loss mean/std: 4.2227 / 5.7773\n",
      "Confidences min/max: 0.2489 / 1.0000\n",
      "Loss min/max: -0.0000 / 16.6406\n",
      "ðŸ“‰ (confidence vs. token-level loss): -0.239\n",
      "\n",
      "ðŸ” Loading model: openai-community/gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#valid tokens: 84\n",
      "Confidences mean/std: 0.3237 / 0.2864\n",
      "Loss mean/std: 4.0703 / 3.2754\n",
      "Confidences min/max: 0.0128 / 0.9961\n",
      "Loss min/max: 0.0039 / 14.1484\n",
      "ðŸ“‰ (confidence vs. token-level loss): -0.542\n",
      "\n",
      "ðŸ” Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cc6bc6a71745279b7a88cafaaa720f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#valid tokens: 103\n",
      "Confidences mean/std: 0.6030 / 0.3210\n",
      "Loss mean/std: 2.4609 / 3.2676\n",
      "Confidences min/max: 0.0200 / 1.0000\n",
      "Loss min/max: -0.0000 / 15.9453\n",
      "ðŸ“‰ (confidence vs. token-level loss): -0.541\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import pearsonr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "def compute_confidence_loss_correlation(model, tokenizer, text):\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        logits = outputs.logits\n",
    "\n",
    "    shifted_logits = logits[:, :-1, :]\n",
    "    shifted_labels = inputs[\"input_ids\"][:, 1:]\n",
    "    probs = F.softmax(shifted_logits, dim=-1)\n",
    "\n",
    "    confidences, _ = probs.max(dim=-1)\n",
    "    true_token_probs = probs.gather(2, shifted_labels.unsqueeze(-1)).squeeze(-1)\n",
    "    loss_per_token = -torch.log(true_token_probs + 1e-12)\n",
    "\n",
    "    confidences_np = confidences.flatten().cpu().numpy()\n",
    "    loss_np = loss_per_token.flatten().cpu().numpy()\n",
    "\n",
    "    # ðŸ”§ Filter invalid values\n",
    "    valid_mask = np.isfinite(loss_np)\n",
    "    confidences_np = confidences_np[valid_mask]\n",
    "    loss_np = loss_np[valid_mask]\n",
    "\n",
    "    print(f\"#valid tokens: {len(confidences_np)}\")\n",
    "    print(f\"Confidences mean/std: {np.mean(confidences_np):.4f} / {np.std(confidences_np):.4f}\")\n",
    "    print(f\"Loss mean/std: {np.mean(loss_np):.4f} / {np.std(loss_np):.4f}\")\n",
    "    print(f\"Confidences min/max: {confidences_np.min():.4f} / {confidences_np.max():.4f}\")\n",
    "    print(f\"Loss min/max: {loss_np.min():.4f} / {loss_np.max():.4f}\")\n",
    "\n",
    "    if len(confidences_np) < 2 or np.std(confidences_np) == 0 or np.std(loss_np) == 0:\n",
    "        print(\"âš ï¸ Not enough valid data for correlation.\")\n",
    "        return float(\"nan\"), confidences_np, loss_np\n",
    "\n",
    "    corr, _ = pearsonr(confidences_np, loss_np)\n",
    "    return corr, confidences_np, loss_np\n",
    "\n",
    "\n",
    "# ðŸ”¬ Input prompt (longer for better token count)\n",
    "text = (\n",
    "    \"The genes ['ENSG00000283959', 'KLKP1-1', 'BBOX1-AS1'] are upregulated in this sample. \"\n",
    "    \"We are interested in evaluating the potential therapeutic impact of Dabrafenib on these genes. \"\n",
    "    \"Provide a detailed mechanistic rationale for how Dabrafenib might affect downstream signaling pathways, \"\n",
    "    \"especially focusing on any known or predicted interactions with MAPK signaling.\"\n",
    ")\n",
    "\n",
    "# âœ… Assume google/txgemma-2b-chat is already loaded\n",
    "print(\"\\nðŸŸ¢ Computing for pre-loaded model: google/txgemma-2b-chat\")\n",
    "corr, confidences_np, loss_np = compute_confidence_loss_correlation(model, tokenizer, text)\n",
    "\n",
    "if not np.isnan(corr):\n",
    "    print(f\"ðŸ“‰ (confidence vs. token-level loss): {corr:.3f}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Correlation computation failed for google/txgemma-2b-chat\")\n",
    "\n",
    "# ðŸ“¦ List of other models to compare\n",
    "model_ids = [\n",
    "    \"openai-community/gpt2\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "]\n",
    "\n",
    "# ðŸš€ Compare each model\n",
    "for model_id in model_ids:\n",
    "    try:\n",
    "        print(f\"\\nðŸ” Loading model: {model_id}\")\n",
    "        \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True\n",
    "        )\n",
    "        \n",
    "        model_tmp = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=bnb_config,\n",
    "        )\n",
    "        tokenizer_tmp = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        corr, confidences_np, loss_np = compute_confidence_loss_correlation(model_tmp, tokenizer_tmp, text)\n",
    "        \n",
    "        if np.isnan(corr):\n",
    "            continue \n",
    "        \n",
    "        print(f\"ðŸ“‰ (confidence vs. token-level loss): {corr:.3f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed for model {model_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f07df55e-40d3-46f5-af04-599117a17381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instructions: Answer the following question about drug properties.\n",
      "Context: DTP3 is a small molecule drug\n",
      "Question: Find me the drug information about this DTP3\n",
      "Answer: DTP3 is an experimental drug. There is no publicly available information about its properties, mechanisms of action, or clinical trials. \n",
      "\n",
      "\n",
      "Instructions: Answer the following question about drug involvement in pathway.\n",
      "Context: DTP3 is an experimental drug. There is no publicly available information about its properties, mechanisms of action, or clinical trials.\n",
      "Question: What does DTP3 do? What are the upregulated and downregulatedpathways it affects.\n",
      "Answer: This question cannot be answered. There is no publicly available information about DTP3, its mechanisms of action, or the pathways it affects.\n"
     ]
    }
   ],
   "source": [
    "drug = \"DTP3\" \n",
    "\n",
    "# Stage 1: Get drug information\n",
    "prompt = f\"\"\"\n",
    "Instructions: Answer the following question about drug properties.\n",
    "Context: {drug} is a small molecule drug\n",
    "Question: Find me the drug information about this {drug}\n",
    "Answer:\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "\n",
    "# Extract drug information from the first response\n",
    "drug_info = response.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Stage 2: Get pathway involvement\n",
    "prompt = f\"\"\"\n",
    "Instructions: Answer the following question about drug involvement in pathway.\n",
    "Context: {drug_info}\n",
    "Question: What does {drug} do? What are the upregulated and downregulatedpathways it affects.\n",
    "Answer:\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64276b08-cf63-41b7-aa04-3cb3797631cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6731a729982a4ab7ae2244e07c0dd6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A549 cells treated with DTP3 show the following pathway changes:\n",
      "- Upregulated: apoptosis, p53 signaling, oxidative stress response\n",
      "- Downregulated: NF-ÎºB signaling, cell cycle, DNA replication\n",
      "\n",
      "DTP3 is known to inhibit NF-ÎºB. Do these pathway changes support the expected mechanism of action? Explain.\n",
      "**\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "Yes, the pathway changes observed in A549 cells treated with DTP3 support its known mechanism of action as an NF-ÎºB inhibitor. Here's why:\n",
      "\n",
      "* **NF-ÎºB inhibition:** DTP3 directly inhibits NF-ÎºB, a transcription factor crucial for inflammation, cell survival, and proliferation. The observed downregulation of NF-ÎºB signaling confirms this direct inhibitory effect. \n",
      "\n",
      "* **Apoptosis induction:** NF-ÎºB often suppresses apoptosis. By inhibiting NF-ÎºB, DTP3 removes this suppression, leading to the upregulation of apoptosis pathways. \n",
      "\n",
      "* **p53 signaling activation:**  p53 is a tumor suppressor protein that can induce apoptosis and cell cycle arrest. NF-ÎºB can negatively regulate p53. Therefore, inhibiting NF-ÎºB with DTP3 would likely release this inhibition, leading to p53 pathway activation.\n",
      "\n",
      "* **Oxidative stress response:** NF-ÎºB plays a role in regulating the cellular response to oxidative stress. Inhibiting NF-ÎºB can disrupt this response, potentially leading to an increase in oxidative stress markers and activation of the oxidative stress response pathway.\n",
      "\n",
      "* **Downregulation of cell cycle and DNA replication:**  NF-ÎºB promotes cell cycle progression and DNA replication.  By inhibiting NF-ÎºB, DTP3 would be expected to suppress these processes, contributing to the observed downregulation.\n",
      "\n",
      "**In\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/txgemma-27b-chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "chat = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"\"\"\n",
    "MIA PaCa-2 cells treated with DTP3 show the following pathway changes:\n",
    "- Upregulated: apoptosis, p53 signaling, oxidative stress response\n",
    "- Downregulated: NF-ÎºB signaling, cell cycle, DNA replication\n",
    "\n",
    "DTP3 is known to inhibit NF-ÎºB. Do these pathway changes support the expected mechanism of action? Explain.\n",
    "\"\"\"\n",
    "\n",
    "response = chat(prompt, max_new_tokens=300)[0][\"generated_text\"]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
